{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment 3\n",
    "\n",
    "Student: Weiyi Chen, weiyi.chen@baruchmail.cuny.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "As in class, suppose you are confronted with a security’s return series $Y$ and you build a neural network model to predict the series. You’ve used regularization techniques and are reasonably assured that your model generalizes well, and that, therefore, its $R^2$ is very close to the optimum $\\max R^2$. We saw in class that if you use the model’s return prediction to build a trading strategy, and assume that the expected return of your strategy is proportional to the volatility of the predictable portion of the return series, $r=\\lambda \\sigma_Y$􏰺􏰻, then the strategy’s Sharpe Ratio is:\n",
    "\n",
    "$$ SR \\propto \\lambda R $$\n",
    "\n",
    "Confirm that this is true by using the fact that the volatility in the denominator of the Sharpe Ratio is the full (noisy) signal’s volatility and not just the volatility of the predictable (target) signal. Note also that this is the *unadjusted* Sharpe Ratio (i.e., we are not subtracting the risk-free rate from the expected return)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "#### SR and Signal-to-Noise\n",
    "\n",
    "If we assume that our model\n",
    "\n",
    "- Has not been overfitted\n",
    "- Has captured the predictive part of the signal reasonably well (i.e., its $R^2$􏱀􏰔 is close optimal)\n",
    "\n",
    "We can make inferences about the signal-to-noise ratio $\\frac{1}{2}$ and hence about the model’s risk. For example,\n",
    "\n",
    "$$ R^2 \\propto \\max R^2 = 1 - \\frac{n^2}{1+n^2} \\Rightarrow n^2 \\propto \\frac{1-R^2}{R^2} $$\n",
    "\n",
    "If we assume that the model’s expected return is proportional to the (unobserved) target signal’s volatility,\n",
    "\n",
    "$$ r \\propto \\lambda \\sigma_Y $$\n",
    "\n",
    "Then we can see that the Sharpe Ratio of a (single-security) strategy employing our model is\n",
    "\n",
    "$$ SR \\propto \\frac{r}{\\sqrt{1+n^2} \\sigma_Y} \\propto \\frac{\\lambda}{\\sqrt{1+n^2}} \\propto \\lambda R $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "- a. Show that the Random Classifier traces the Random Classifier Line in the ROC curve.\n",
    "- b. Show that the Youden Index, J, of a given classifier is equivalent to the Euclidean Distance from the given classifier to the Random Classifier Line. In other words, J always ranks any two classifiers in exactly the same way as the Euclidean Distance to the Random Classifier Line would.\n",
    "- c. Show that the Euclidean Distance Accuracy, *DistAcc*, the normalized Euclidean distance from a given classifier to the Perfect Classifier, is not equivalent to the distance from the given classifier to the Random Classifier Line.\n",
    "- d. Construct two example classifiers A and B (as coordinates on a ROC plot) such that the Euclidean Distance Accuracy, DistAcc will rank A as better than B, while the Cost-Adjusted Euclidean Distance Accuracy, with a given W, will revert the ranking found using DistAcc. Explain how this can happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "#### (a) Random Classifier Line\n",
    "\n",
    "The Random Classifier Line in the ROC curve is $y = x$. A Random Classifier is directly using prior probability to guess the positive, let's assume $p$ as the prior probability. Given the outcomes are positive, we have probability $p$ to guess it right, while given the outcomes are negative, we still have probability $p$ to guess it positive, which implies that the true positive rate $p$ is associated with the false positive rate $p$. Thus this forms the random clssifier line $y=x$.\n",
    "\n",
    "#### (b) Youden Index\n",
    "\n",
    "<img src=\"ROC.png\" width=400px>\n",
    "\n",
    "The Youden Index 􏱋$J$ is often used as the optimal criterion for choosing the threshold of a classifier:\n",
    "\n",
    "$$ J = tp - fp = \\text{sensitivity} - (1 - \\text{specificity}) $$\n",
    "\n",
    "And the Euclidean Distance from the given classifier to the Random Classifier Line, as indicated in the graph, is the distance between point $(fp, tp)$ and $(fp, fp)$, i.e. $(tp - fp)$, the same as Youden Index.\n",
    "\n",
    "#### (c) Euclidean Distance Accuracy\n",
    "\n",
    "The Euclidean Distance Accuracy is\n",
    "\n",
    "$$ \\text{DistAcc} = 1 - \\sqrt{\\frac{1}{2}\\left[(1-tp)^2 + fp^2\\right]}$$\n",
    "\n",
    "which is not necessarily equal to the Euclidean Distance from the given classifier to the Random Classifier Line $(tp - fp)$.\n",
    "\n",
    "#### (d) Cost-Adjusted Euclidean Distance Accuracy\n",
    "\n",
    "The Cost-Adjusted Euclidean Distance Accuracy is \n",
    "\n",
    "$$ \\text{Cost-Adjusted DistAcc} = 1 - \\sqrt{W(1-tp)^2 + (1-W)fp^2} $$\n",
    "\n",
    "Let's construct two examples \n",
    "\n",
    "$$ A = (0.5, 0.5), B = (0.4, 0.4) $$ \n",
    "\n",
    "and set the weight of cost-adjusted one as $0.3$. Below is the calculation of their distance accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Accuracy of A:\t\t\t0.5\n",
      "Distance Accuracy of B:\t\t\t0.490098048641\n",
      "Cost Adjusted Distance Accuracy of A:\t0.5\n",
      "Cost Adjusted Distance Accuracy of B:\t0.530958424018\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def Cost_adjusted_DistAcc(W, t):\n",
    "    fp, tp = t\n",
    "    return 1 - math.sqrt(W * (1-tp)**2 + (1-W) * fp**2)\n",
    "\n",
    "def DistAcc(t):\n",
    "    return Cost_adjusted_DistAcc(0.5, t)\n",
    "\n",
    "A = (0.5, 0.5)\n",
    "B = (0.4, 0.4)\n",
    "W = 0.3\n",
    "print 'Distance Accuracy of A:\\t\\t\\t', DistAcc(A)\n",
    "print 'Distance Accuracy of B:\\t\\t\\t', DistAcc(B)\n",
    "print 'Cost Adjusted Distance Accuracy of A:\\t', Cost_adjusted_DistAcc(W, A)\n",
    "print 'Cost Adjusted Distance Accuracy of B:\\t', Cost_adjusted_DistAcc(W, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This happens because the cost is heavily incurred on false positive rate $fp$. With a larger fp for A, it is unfavorably ranked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "In class we considered a special classifier used to decide whether to enter a trade. Class 1 represents \"The Trade is Profitable\" and Class 0 represents \"The Trade is Unprofitable.\" The profit of entering a profitable trade $P$ is 􏱃 while the loss of entering an unprofitable trade is $L$􏱻. The classifier’s output $o$􏱩 ranges from 0 to 1 and can be considered a proxy for the probability that Class 1 is true. From this, we saw that setting the classifier threshold 􏱼$\\theta$ to\n",
    "\n",
    "$$ \\theta = \\frac{L}{L + P} $$\n",
    "\n",
    "was equivalent to entering the trade when the expected profit exceeded the expected loss. We also noted that in HFT, 􏱻 $L$ is usually larger than $P$􏱃. What simple condition does this impose on $\\theta$􏱼?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "In HFT usually $L>P$ due to transaction costs and other frictions, then\n",
    "\n",
    "$$ \\theta = \\frac{L}{L+P} > 0.5 $$\n",
    "\n",
    "therefore the classifier threshold is larger than 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "Using the same classifier as in 2. above, explain the following two equations:\n",
    "\n",
    "$$ \\text{Expected Profit} = TP \\cdot P \\\\ \n",
    "\\text{Expected Loss} = FP \\cdot L + FN \\cdot P $$\n",
    "\n",
    "Here we have assumed that􏱃 $TP$ is the fraction of all classifications that were correctly identified as Class 1 by the classifier, $FP$􏱾􏱃 is the fraction of al classifications that were incorrectly identified as Class 1, and 􏱾􏱆$FN$ is the fraction of all classifications that were incorrectly identified as Class 0. Again requiring that the Expected Profit exceed the Expected Loss, we get a relation between classifier performance (the left-hand side, in terms of 􏱞􏱃$TP, FP􏱾􏱆, FN$􏱾􏱃) and the Profit/Loss ratio (in the right-hand side):\n",
    "\n",
    "$$ \\frac{TP - FN}{FP} > \\frac{L}{P} $$\n",
    "\n",
    "Discuss why this relation makes sense. (What happens as the Loss becomes larger in relation to the Profit? What happens to the classifier performance as $TP$􏱞􏱃 increases, etc?)\n",
    "\n",
    "Discuss how this relation compares with the Youden Index as a classifier performance measure. Discuss why, in this particular case, the True Negatives (􏱞􏱆$TN$) do not enter the expression for classifier performance (the right-hand side of the above equation).\n",
    "\n",
    "Given that $􏱻L>P$, are False Positives (Type I Errors) more or less costly than False Negatives (Type II Errors)? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: Misclassification Costs\n",
    "\n",
    "#### Expected profit and loss\n",
    "\n",
    "We can see that Profit will only result from entering a Profitable Trade:\n",
    "\n",
    "$$ \\text{Expected Profit} = TP \\cdot P $$\n",
    "\n",
    "where $P$ 􏱃is the Profit associated with entering a Profitable Trade. A Loss can occur either from entering an Unprofitable Trade, or from not entering a potentially Profitable Trade:\n",
    "\n",
    "$$ \\text{Expected Loss} = FP \\cdot L + FN \\cdot P $$\n",
    "\n",
    "where $L$ is the Loss associated with entering an Unprofitable Trade.\n",
    "\n",
    "#### Profit / Loss ratio\n",
    "\n",
    "The condition for profitability is that the Expected Profit exceed the Expected Loss:\n",
    "\n",
    "$$ TP \\cdot P > FP \\cdot L + FN \\cdot P $$\n",
    "\n",
    "This imposes a condition on the classifier’s performance (and misclassification costs) given the trade’s profit and loss:\n",
    "\n",
    "$$ \\frac{TP - FN}{FP} > \\frac{L}{P} $$\n",
    "\n",
    "- As $TP$ increases, which implies we are more likely to enter profitable trades, the inequality is easier to satisfy, which implies we would trade more active.\n",
    "- As $FN$ decreases, which implies we are less likely to miss profitable trades, the inequality is easier to satisfy, which implies we would be more active.\n",
    "- As $FP$ decreases, which implies we are less likely to enter unprofitable trades, the inequality is easier to satisfy, which implies we would be more active.\n",
    "- As $L$ decreases, which implies the loss associated with entering each Unprofitable Trade is less, the inequality is easier to satisfy, which implies we would be more active.\n",
    "- As $P$ increases, which implies the profit associated with entering each profitable trade is greater, the inequality is easier to satisfy, which implies we would be more active.\n",
    "\n",
    "#### Vs. Youdan\n",
    "\n",
    "This relation is imposed on more parameters, i.e. true positives, false negatives and false positives, loss and profit in each trade, which is more practical and intuitive according to the analysis above.\n",
    "\n",
    "The True Negatives (􏱞􏱆TN) do not enter the expression for classifier performance, because it refers to the trades are not profitable and we correctly classify them as negative, which does not incur any loss/profit when making such a decision.\n",
    "\n",
    "#### Type I Errors vs. Type II Errors\n",
    "\n",
    "False Positives (Type I Errors) are more costly than False Negatives (Type II Errors). We can convert the equation as\n",
    "\n",
    "$$ \\frac{(TP - FN)\\times P}{FP \\times L} > 1 $$\n",
    "\n",
    "Give $L>P$, the effectiveness of $FP$ would be larger than $FN$. Practically speaking, if loss is larger than profit in each trade, we should be more careful entering an unprofitable trade (i.e. $FP$), compared to miss a profitable trade (i.e. $FN$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "You are designing a classifier to predict whether a trade will yield $+2$, $+1$, $0$, $-1$, or $-2$ cents per stock traded. This type of classifier can be called an “Ordinal Classifier” because the classes can be ranked in order, where class $+2$ (representing a trade that yields a profit of $2$ cents) is more distant from class $-1$ (representing a trade that yields a loss of $1$ cent), than it is to class $+1$. You trained two classifiers, A and B, and tested each of them on the same hold-out data set, which produced the following two (count) Confusion Matrices\n",
    "\n",
    "<img src=\"confusion_matrix.png\" width=300px>\n",
    "\n",
    "Ignoring transaction costs, discuss how you would decide which classifier is better. Would a diagonality measure be useful as the sole criterion for comparison? Why or why not? How many pair-wise binary classifiers would you have to analyze if you were to make a pair-wise comparison among the classes? Would you treat all such pair-wise comparisons the same way to arrive at your overall comparison between classifiers A and B? How many pair-wise comparisons would you have to make if you had N classes instead of 5? Would your analysis change if you added asymmetric transaction costs (where entering a trade is always more expensive than not trading)? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "A diagonality measure is not useful as the sole criterion for comparison in this case, because both of them are 5288, we cannot tell which classifier is better.\n",
    "\n",
    "The number of pair-wise binary classifiers we have to analyze, if you were to make a pair-wise comparison among the classes, is \n",
    "\n",
    "$$ \\frac{5 \\times 4}{2} = 10 $$\n",
    "\n",
    "I would not treat all such pair-wise comparisons the same way to arrive at your overall comparison between classifiers A and B, because the costs of differnt misclassifications are different. For example, $-2$ misclasified to class $+2$ is more serious than $+1$ misclasified to class $+2$.\n",
    "\n",
    "If we had N classes instead of 5, the number of pair-wise comparisons would be \n",
    "\n",
    "$$ \\frac{N(N-1)}{2} $$\n",
    "\n",
    "My analysis will change if you added asymmetric transaction costs, it's like $L$ increases while $P$ decreases, the inequality in *problem 4* is harder to satisfy, which implies we would trade less active."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "\n",
    "Discuss the advantages and disadvantages of the five Data Imputation methods that we discussed within our European Companies Case Study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "#### Non-null Attribute Mean (or Median) Imputation\n",
    "\n",
    "The Mean (Median) Imputation is not very accurate, but it’s easy and cheap to implement even on large data corpuses. It is easily parallelizable as well (SIMD).\n",
    "\n",
    "#### Hot-Deck Imputation\n",
    "\n",
    "Hot-Deck Imputation is commonly used, as it is fairly accurate, relatively cheap to implement, and parallelizable (SIMD).\n",
    "\n",
    "#### KNN and LSR Imputation\n",
    "\n",
    "KNN and LSR are more accurate than Hot-Deck and give good results, but are more expensive to implement, although they are parallelizable. Moreover, Hot-Deck, KNN, and LSR only draw on part of the data (the subset of complete records), which can be significantly smaller than the original data set. In our example, there are 576 complete records out of the original 6,022 — a small fraction of the original data set. This could significantly bias our data.\n",
    "\n",
    "#### SVD Imputation\n",
    "\n",
    "SVD Imputation gives reasonably good results and it has the added advantage of using all of the available data. However, it is expensive to implement, and is suitable only for small to medium-sized data sets. It is not easily parallelizable due to the relative norm computation at each iteration (although the SVD computation itself is parallelizable).\n",
    "\n",
    "A drawback of SVD Imputation is its reliance on the ad-hoc parameters $\\Delta$ and $􏱔\\delta$. Another drawback is the fact that it is not self-starting, relying on another imputation method to bootstrap the first iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7\n",
    "\n",
    "Suppose you’re searching for Association Rules in a transaction database with 1,000,000 transactions. Your algorithm is using the Support-Confidence Framework, and you have decided to require a Support lower bound of 􏰐$s = 1\\%$ and a confidence level of no less than $􏱽c = 10\\%$. You’re considering Itemsets $X$􏱣 and $Y$􏰹 and have measured the following occurrences in the database: Number of occurrences of $X$􏱣 is 500,000; $Y$􏰹 occurs 400,000 times; $X$ and $Y$􏰹 together occur 200,000 times. \n",
    "\n",
    " - (a) What is the support of the rule $X\\Rightarrow Y$? Does it meet the support threshold? \n",
    " - (b) What is the confidence of the rule 􏱣$X \\Rightarrow Y$􏰹? Does it meet the confidence threshold? \n",
    " - (c) Based on your answers to (a) and (b), is this an Interesting Association Rule according to the Support-Confidence Framework? Discuss whether this is a good idea. \n",
    "\n",
    "Now suppose that $X$􏱣 and $Y$􏰹 occur together only 2,000 times. \n",
    "\n",
    " - (d) What is the support of the rule $X\\Rightarrow Y$􏱣􏰹? Does it meet the support threshold? \n",
    " - (e) What is the confidence of the rule $X\\Rightarrow Y$ and does it meet the confidence threshold? \n",
    " - (f) Based on the answers to (e) and (f), is this an Interesting Association Rule according to the Support-Confidence Framework? Discuss whether this is a good idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "#### (a) Support\n",
    "\n",
    "The support of the rule $X \\Rightarrow Y$ is \n",
    "\n",
    "$$ \\frac{200,000}{1,000,000} = 20\\% > s $$\n",
    "\n",
    "which meet the support threshold.\n",
    "\n",
    "#### (b) Confidence\n",
    "\n",
    "The confidence of the rule $X \\Rightarrow Y$ is \n",
    "\n",
    "$$ \\frac{200,000}{500,000} = 40\\% > c $$\n",
    "\n",
    "which meet the confidence threshold.\n",
    "\n",
    "#### (c) Conclusion\n",
    "\n",
    "Yes it is an Interesting Association Rule according to the Support-Confidence Framework. But it's not an good idea as you could figure out\n",
    "\n",
    "$$ P(A \\cap B) = P(A) \\times P(B) $$\n",
    "\n",
    "which implies they are independent.\n",
    "\n",
    "#### (d) Support\n",
    "\n",
    "The support of the rule $X \\Rightarrow Y$ is \n",
    "\n",
    "$$ \\frac{2,000}{1,000,000} = 0.2\\% < s $$\n",
    "\n",
    "which does not meet the support threshold.\n",
    "\n",
    "#### (e) Confidence\n",
    "\n",
    "The confidence of the rule $X \\Rightarrow Y$ is \n",
    "\n",
    "$$ \\frac{2,000}{500,000} = 0.4\\% < c $$\n",
    "\n",
    "which does not meet the confidence threshold.\n",
    "\n",
    "#### (f) Conclusion\n",
    "\n",
    "No it is not an Interesting Association Rule according to the Support-Confidence Framework. But it's not an good idea because X and Y occurs together less frequently than the independent situation. However it's possible they have a negative relationship."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
