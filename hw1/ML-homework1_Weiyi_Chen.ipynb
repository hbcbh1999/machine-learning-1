{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment 1\n",
    "\n",
    "Student: Weiyi Chen, weiyi.chen@baruchmail.cuny.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "Show that the single-Perceptron learning rule with hard limit activation converges (for linearly- separable problems). Is this an example of Supervised or Unsupervised Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "#### Perceptron Architecture\n",
    "\n",
    "We use the same notation as page 20 in the slide, there are $R$ inputs, $S$ outputs, $W$ is an $S \\times R$ matrix of weights, $b$ is an $S$ vector of biases.\n",
    "\n",
    "<img src=\"Perception_architecture.png\" width=200px>\n",
    "\n",
    "In Supervised Learning the Training Set contains targets: $(\\textbf{p}_1, \\textbf{t}_1), (\\textbf{p}_2,\\textbf{t}_2), \\dots , (\\textbf{p}_Q, \\textbf{t}_Q)$\n",
    "\n",
    "#### Perception Learning Rule\n",
    "Minimize the error $\\textbf{e}$ between actual response $\\textbf{a}$ and desired target $\\textbf{t}$:\n",
    "\n",
    "$$ \\textbf{e} = \\textbf{t} - \\textbf{a} $$\n",
    "\n",
    "We start with an arbitrary set of weights, then update weights and biases according to\n",
    "\n",
    "$$ \\textbf{W}_{new} = \\textbf{W}_{old} + \\textbf{ep}^T, \\textbf{b}_{new} = \\textbf{b}_{old} + \\textbf{e} $$\n",
    "\n",
    "where the update happens only when $\\textbf{e}$ is either $1$ or $-1$. Denote $\\textbf{X} = (\\textbf{W}, \\textbf{b})^T$, then the learning rule is\n",
    "\n",
    "$$ \\textbf{X}(k) = \\textbf{X}(k-1) + \\textbf{e}(k-1) \\times (\\textbf{p}^T, \\textbf{1})^T $$\n",
    "\n",
    "To show that the single-Perceptron learning rule with hard limit activation converges, and this is a linearly-separable problem, there should be a right weight and bias vector $\\textbf{X}$ that correctly categorizes all input vectors (assume the threshold activation of the hard limit function is $\\delta$),\n",
    "\n",
    "$$ \\textbf{a}_q^* = \\textbf{X}^T (\\textbf{p}_q^T, \\textbf{1})^T > \\delta, \\text{ if } t_q = 1 $$\n",
    "$$ \\textbf{a}_q^* = \\textbf{X}^T (\\textbf{p}_q^T, \\textbf{1})^T < -\\delta, \\text{ if } t_q = 0 $$\n",
    "\n",
    "#### Convergence\n",
    "\n",
    "According to the learning rule, we aggregate all the updates, then\n",
    "\n",
    "$$ \\textbf{X}(k) = \\textbf{X}(0) + \\sum_{k-1} \\textbf{e}(i) \\times (\\textbf{p}^T, \\textbf{1})^T $$\n",
    "\n",
    "Multiply $\\textbf{X}^*$ to both sides, we have\n",
    "\n",
    "$$  (\\textbf{X}^*)^T (\\sum_{k-1} \\textbf{e}(k-1) \\times (\\textbf{p}^T, \\textbf{1})^T)  \n",
    "=   (\\textbf{X}^*)^T (\\textbf{X}(k)-\\textbf{X}(0))\n",
    "\\le \\sqrt{||{\\textbf{X}^*}||^2 \\times ||(\\textbf{X}(k)-\\textbf{X}(0))||^2}\n",
    "$$\n",
    "\n",
    "Using $(\\textbf{X}^*)^T (\\sum_{k-1} \\textbf{e}(k-1) \\times (\\textbf{p}^T, \\textbf{1})^T) > k\\delta$, we derive\n",
    "\n",
    "$$ ||(\\textbf{X}(k)-\\textbf{X}(0))||^2 > \\frac{k^2\\delta^2}{||{\\textbf{X}^*}||^2} $$\n",
    "\n",
    "Since $\\textbf{X}(0)$ is arbitrary, for simlicity, let's assume $\\textbf{X}(0) = \\textbf{0}$, then we derive the lower bound for weight and bias vector,\n",
    "\n",
    "$$ ||(\\textbf{X}(k)||^2 > \\frac{k^2\\delta^2}{||{\\textbf{X}^*}||^2} $$\n",
    "\n",
    "To derive the upper bound, we use the learning rule and ignore cross terms inside $||(\\textbf{X}(k)||^2$, that is\n",
    "\n",
    "$$  ||(\\textbf{X}(k)||^2 = || \\textbf{X}(k-1) + \\textbf{e}(k-1) \\times (\\textbf{p}^T, \\textbf{1})^T || ^ 2\n",
    "\\le ||\\textbf{X}(k-1)||^2 + || \\textbf{e}(k-1) \\times (\\textbf{p}^T, \\textbf{1})^T || ^ 2\n",
    "\\le ||\\textbf{X}(0)||^2 + \\sum_{k-1}|| \\textbf{e}(i) \\times (\\textbf{p}^T, \\textbf{1})^T || ^ 2 $$\n",
    "\n",
    "Again for simlicity, let's assume $\\textbf{X}(0) = \\textbf{0}$,\n",
    "\n",
    "$$ ||(\\textbf{X}(k)||^2 \\le \\sum_{k-1}|| \\textbf{e}(i) \\times (\\textbf{p}^T, \\textbf{1})^T || ^ 2 < kM$$\n",
    "\n",
    "where $M = \\max{|| \\textbf{e}(i) \\times (\\textbf{p}^T, \\textbf{1})^T || ^ 2}$, putting upper and lower bounds together, we derive\n",
    "\n",
    "$$ k < \\frac{M||\\textbf{X}^*||^2}{\\delta^2} $$\n",
    "\n",
    "which implies the single-Perceptron learning rule with hard limit activation converges, this is an example of supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Show x XOR y = (x OR y) AND [NOT(x AND y)] (truth tables are good enough)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "| x | y | x OR y | NOT(x AND y) | (x OR y) AND [NOT(x AND y)]\n",
    "| - | - | :----: | :----------: | :-------------------------:\n",
    "| 0 | 0 | 0      | 1            | 0\n",
    "| 0 | 1 | 1      | 1            | 1\n",
    "| 1 | 0 | 1      | 1            | 1\n",
    "| 1 | 1 | 1      | 0            | 0\n",
    "\n",
    "which is the same as\n",
    "\n",
    "<img src=\"XOR.png\" width=200px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Why are the outputs of the OR and the NOT-AND Perceptrons guaranteed to be linearly\n",
    "separable? Does this hold in higher dimensions? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "<img src=\"AND-OR.png\" width=500px>\n",
    "\n",
    "As shown above, the outputs of the OR and the NOT-AND Perceptrons are guaranteed to be linearly separable because there exist a line, aka Decision Boundary Line, capable to divide the input space into two regions. In higher dimensions, this still holds, the decision boundary line is defined by\n",
    "\n",
    "$$ \\textbf{Wp} + \\textbf{b} = \\textbf{0} $$\n",
    "\n",
    "is actually a Hyperplane in higher dimensions. Below is a formal proof.\n",
    "\n",
    "#### Linear Seprarability Definition\n",
    "\n",
    "Let $X_{0}$ and $X_{1}$ be two sets of points in an $n$-dimensional Euclidean space. Then $X_{0}$ and $X_{1}$ are linearly separable if there exists $n + 1$ real numbers $w_{1}, w_{2},..,w_{n}, k$, such that every point $x \\in X_{0}$ satisfies $\\sum^{n}_{i=1} w_{i}x_{i} > k$ and every point $x \\in X_{1}$ satisfies $\\sum^{n}_{i=1} w_{i}x_{i} < k$, where $x_{i}$ is the i-th component of $x$.\n",
    "\n",
    "#### OR case\n",
    "\n",
    "In OR case ($n$-dimention Euclidean space), denote a function $OR()$ where given input $\\textbf{x} = (x_1, x_2, ..., x_n)^T$,\n",
    "\n",
    "$$ OR(\\textbf{x}) = \\begin{cases} 0, &\\mbox{if } x_1=x_2=...=x_n=0 \\\\\n",
    "1, &\\mbox{otherwise}\n",
    "\\end{cases} $$\n",
    "\n",
    "Let $w_{1} = w_{2} = ... = w_{n} = 1, k = 0.5$, then obviously\n",
    "\n",
    "$$ \\sum^{n}_{i=1} w_{i}\\textbf{x}_{i} = \\begin{cases} < k, &\\mbox{if } x_{i,1}=x_{i,2}=...=x_{i,n}=0  \\\\\n",
    "> k, &\\mbox{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "Therefore the outputs of the OR Perceptron is linearly separable.\n",
    "\n",
    "#### NOT-AND case\n",
    "\n",
    "In NOT-AND case ($n$-dimention Euclidean space), denote a function $NOT-AND()$ where given input $\\textbf{x} = (x_1, x_2, ..., x_n)^T$,\n",
    "\n",
    "$$ NOT-AND(\\textbf{x}) = \\begin{cases} 0, &\\mbox{if } x_1=x_2=...=x_n=1 \\\\\n",
    "1, &\\mbox{otherwise}\n",
    "\\end{cases} $$\n",
    "\n",
    "Let $w_{1} = w_{2} = ... = w_{n} = -1, k = n-0.5$, then obviously\n",
    "\n",
    "$$ \\sum^{n}_{i=1} w_{i}\\textbf{x}_{i} = \\begin{cases} > k, &\\mbox{if } x_{i,1}=x_{i,2}=...=x_{i,n}=1  \\\\\n",
    "< k, &\\mbox{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "Therefore the outputs of the NOT-AND Perceptron is linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "Can you think of a Perceptron architecture that would solve the XOR problem using an MLP\n",
    "with Hard-Limit activations? Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "#### Multi-Layer Perceptron for XOR\n",
    "\n",
    "<img src=\"MLP-XOR.png\" width=500px>\n",
    "\n",
    "This diagram from Slide page 35 has already illustrated a Multi-layer Perceptron with two inputs, two hidden-layer nodes, and one output node. The two hidden nodes are made up of a previously-trained OR single Perceptron and a previously-trained NOT-AND single Perceptron. The output node is a previously-trained single AND Perceptron.\n",
    "\n",
    "This combination should allow the MLP to implement the XOR function as follows:\n",
    "\n",
    "$$ x \\textbf{ XOR } y = (x \\textbf{ OR } y) \\textbf{ AND } [\\textbf{ NOT }(x \\textbf{ AND } y)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "Can you think of a Perceptron learning rule that would converge in an automated fashion for the\n",
    "XOR problem using an MLP with Hard-Limit activations? Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "No. The XOR problem is an example of a broader class of problems: “Classify all edges of a hypercube into two classes (arbitrarily located).” This Linearly Non-Separable Problem cannot be solved with a single line (hyperplane in higher dimensions).\n",
    "\n",
    "The fix proposed in problem 4 (assembling individually trained Perceptrons) is not useful because it doesn’t give us an automated (or even concise) way of solving this combinatorially explosive problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "\n",
    "Derive a MADALINE learning rule for a simple MADALINE with two inputs, two linear-\n",
    "activation nodes in the hidden layer and one linear-activation output node by back-propagating the error from the output node to the input weights using the chain rule of differentiation. The goal is to minimize the squared error: $e = 1/2(t – y)^2$ using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "#### MADALINE layer and notation\n",
    "\n",
    "<img src=\"MADALINE.png\" width=400px>\n",
    "\n",
    "Denote the two inputs as\n",
    "\n",
    "$$ \\textbf{p} = (p_1, p_2)^T $$\n",
    "\n",
    "the two linear-activation nodes in the hidden layer as\n",
    "\n",
    "$$ \\textbf{W} = \\left( \\begin{array}{cc}\n",
    "W_{11} & W_{12} \\\\\n",
    "W_{21} & W_{22} \\end{array} \\right), \\textbf{b} = \\left( \\begin{array}{c}\n",
    "b_{1} \\\\\n",
    "b_{2} \\end{array} \\right)\n",
    "$$\n",
    "\n",
    "these two will result in two hidden activations as\n",
    "\n",
    "$$ \\textbf{a} = \\textbf{Wp} + \\textbf{b} = (a_1, a_2)^T $$\n",
    "\n",
    "Similaly, at the output layer, denode the linear-activation output node as\n",
    "\n",
    "$$ \\textbf{W'} = \\left( \\begin{array}{cc}\n",
    "W'_{1} & W'_{2} \\end{array} \\right), \\textbf{b'} = \\left( \\begin{array}{c}\n",
    "b'_{1} \\end{array} \\right) $$\n",
    "\n",
    "resulting in output as\n",
    "\n",
    "$$ y = \\textbf{W'a} + \\textbf{b'} $$\n",
    "\n",
    "#### ADALINE learning rule\n",
    "\n",
    "For smooth activations, the weight update at the kth iteration is:\n",
    "\n",
    "$$ \\Delta W_k = -\\eta \\triangledown_w e_k $$\n",
    "\n",
    "where $\\eta$ is the Learning Rate.\n",
    "\n",
    "#### MADALINE learning rule\n",
    "\n",
    "Similarly, \n",
    "\n",
    "$$ \\Delta W_k' = - \\eta' \\triangledown_{W'} (e_k) = -\\eta' (t-y_k) (a_{1,k}, a_{2,k}, 1)^T $$\n",
    "\n",
    "$$ \\Delta W_k = -\\eta \\triangledown_{a_k} (e_k) \\triangledown_{W_k}(a_k) = -\\eta (t-y_k) (W'_{1,k}, W'_{2,k}, b'_{1,k})^T (p_{1,k}, p_{2,k}, 1)^T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7\n",
    "\n",
    "Show that MADALINEs with one Hidden Layer cannot solve linearly non-separable two-class problems by demonstrating that the separation hyperplane is given by\n",
    "\n",
    "$$ \\tilde{\\textbf{W}} \\textbf{p} + \\tilde{\\textbf{b}} = \\textbf{0} $$\n",
    "\n",
    "where $\\tilde{\\textbf{W}}$ and $\\tilde{\\textbf{b}}$ are functions of the hidden-layer and output-layer weights and biases. Show that this result holds for an arbitrary number of hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Using the same notation as *Problem 6*. Then at the output node we have\n",
    "\n",
    "$$ W'a + b' = 0 \\Rightarrow W'(Wp+b) + b' = 0 \\Rightarrow W'Wp + W'b + b' = 0$$\n",
    "\n",
    "If we let $\\tilde W = W'W$ and $\\tilde b = W'b + b'$, then we could represent the separation hyperplance is given by\n",
    "\n",
    "$$ \\tilde{\\textbf{W}} \\textbf{p} + \\tilde{\\textbf{b}} = \\textbf{0} $$\n",
    "\n",
    "This cannot solve linearly non-separable two-class problems because a single line cannot divide linearly non-separable inputs into two classes.\n",
    "\n",
    "In general from case of 1 hiddern layer to n hiddern layers,\n",
    "\n",
    "$$ \\tilde{\\textbf{W}} = \\textbf{W'}\\textbf{W}_1\\textbf{W}_2 \\dots \\textbf{W}_n,\n",
    "\\tilde{\\textbf{b}} = \\textbf{W'b}_1 + \\dots + \\textbf{W'b}_n + \\textbf{b}  $$\n",
    "\n",
    "We have already known MADALINEs with one Hidden Layer cannot solve Linearly-Separable two-class problems, now MADALINEs with arbitrary number of hidden layers has reduced to one layer, therefore this result still holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8\n",
    "What are the two criteria required for ANNs to be universal mapping approximators? What\n",
    "condition must be met for automated learning on a multi-layer architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "#### Universal Approximators\n",
    "This means that they can approximate any mapping to an arbitrary degree of accuracy. These Universal Approximators require Hidden Layers (as we hinted at by the MLPs). The second criterion is they require Non-Linear Activations, as hinted at by the SATALINES.\n",
    "\n",
    "For automated learning, they require Smooth (Differentiable) Activations, as with ADALINEs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
