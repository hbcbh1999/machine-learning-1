{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment 5\n",
    "\n",
    "Student: Weiyi Chen, weiyi.chen@baruchmail.cuny.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "You’re searching for Association Rules in a large transaction database with about 100 million records. You’ve decided to partition your database. You do not wish your Minterm Support error to exceed one tenth of 1%, with a 99% confidence. Ignoring the bias introduced by not sampling with replacement, how many database partitions should you examine? As always, you want the global Minterm Support to stay around 5 in order for your 􏲃􏰔$\\chi^2$ statistic to be robust. Given the size of your partitions, would you have to reduce the Support threshold significantly when searching for Supported Itemsets within the partitions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If 􏰚􏲚$m_s$ is chosen such that\n",
    "\n",
    "$$ m_s \\ge \\frac{1}{2\\epsilon^2} \\ln(\\frac{2}{\\delta}) $$\n",
    "\n",
    "then $P(\\epsilon(X,m_S) > \\epsilon) \\le \\delta$.\n",
    "\n",
    "In our case, let $\\delta = 0.01, \\epsilon = 0.1\\%$, then\n",
    "\n",
    "$$ m_s \\ge \\frac{1}{2\\epsilon^2} \\ln(\\frac{2}{\\delta}) = 2649159$$\n",
    "\n",
    "Ignoring the bias introduced by not sampling with replacement, the number of database partitions is\n",
    "\n",
    "$$ N = 100,000,000 / m_s = 38 $$\n",
    "\n",
    "Given the size of my partitions, i.e. $m_s = 2649159$, the new support threshold would be\n",
    "\n",
    "$$ \\epsilon_{new} = \\sqrt{\\frac{1}{m_s} \\ln(\\frac{1}{\\delta})} = 0.0013 \\ll s = 5 $$\n",
    "\n",
    "I don't have to reduce the Support threshold significantly when searching for Supported Itemsets within the partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.7478286505\n"
     ]
    }
   ],
   "source": [
    "m_s = 2649159\n",
    "N = 100000000. / m_s\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0013184656805817568"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.sqrt(1./m_s * math.log(1/0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "To construct an XOR classifier with two inputs, would you use a Radial Basis Function Neural Network or a Feed-Forward one? Why? Given that the XOR problem has Maximal Linear Complexity, would your answer change if you needed an XOR classifier with a 10-dimensional input space? Why or why not? Would you use the input dimension or the Linear Separability Index to decide between using a RBF-NN vs. a FNN? Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "To construct an XOR classifier with two inputs, I would use a Radial Basis Function Neural Network, it's simpler and much faster learning compared to a Feed-Forward one. Simpler is because there is only one Hidden Layer and it's also a Universal Approximator. Faster than BackPropagation is because it trains $\\mathbf{W}$ directly by set $\\mathbf{t} = \\mathcal{G}\\mathbf{W}$, the only problem is just inverting the activation matrix $\\mathcal{G}$ to solve for $\\mathbf{W}$.\n",
    "\n",
    "My answer would change if the XOR classifier goes to a 10-dimensional input space because RBF performs pretty bad after a few dimensions, which is the problem of Curse of Dimentionality. \n",
    "\n",
    "I would use Linear Separability Index. We can show that every problem that’s not linearly separable in a given dimension, is guaranteed to be linearly separable in a higher dimension. At worst, we can introduce an extra dimension for every class to guarantee Linear Separability (though this is usually overkill). We can refer to the minimum number of dimensions required for linear separability as the Linear Separability Index. The Linear Separability Index is always less than or equal to one plus the number of classes. When they are equal, we can say that the input space has Maximal Linear Complexity. Therefore the Linear Separability is more correlated to convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Can you suggest a way to use an Auto-Associative FNN to approximate the Linear Separability Index of an (input) space? Would this alleviate the Curse of Dimensionality? Can you suggest a way that an Auto-Associative FNN can be used in conjunction with an RBF-NN to improve overall performance? Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Solution\n",
    "\n",
    "The Linear Separability Index is always less than or equal to one plus the number of classes. When they are equal, we can say that the input space has Maximal Linear Complexity. Therefore we could use an Auto-Associative FNN to train with minimal number of hidden nodes while reproducing the inputs  well enought. This input space can be estimated as so-called Maximal Linear Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "Given two arbitrary Perceptrons solving the same Linearly-Separable problem, show that the Perceptron with least-energy weights will generally have better classification performance. If you are using an FNN instead of a Perceptron as a classifier, can you suggest how to modify the penalty (error) function to train the FNN to produce a higher-performing classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "As in the Perceptron case, the classification line (hyperplane) is defined by:\n",
    "\n",
    "$$ \\mathbf{wx} + b = \\mathbf{0} $$ \n",
    "\n",
    "And this is a linearly-separable problem, what perceptron need to do is considering the least-energy weights since it has already reached the format of best classification. At FNN side, it still requires training process via penalty function, which might work worse if we do not modify the penalty function.\n",
    "\n",
    "If we are using an FNN as a classifier, knowing that the weight vector is perpendicular to the classifier line,\n",
    "\n",
    "$$ R = \\frac{1}{2\\sqrt{\\mathbf{w} \\cdot \\mathbf{w}}} $$\n",
    "\n",
    "We can thus minimize a tradeoff between the weight norm and misclassification:\n",
    "\n",
    "$$ \\mathbf{w} \\cdot \\mathbf{w} + \\lambda \\sum_i \\epsilon_i $$\n",
    "\n",
    "where $\\epsilon_i$ are misclassification errors, and $\\lambda$ is a tradeoff parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "Given a noisy return series $Y_i^*$, you train a model to extract a prediction $\\hat Y_i$􏰹􏱁. You improve this 􏰲􏰲prediction further by bootstrapping many models to produce an improved prediction 􏰹􏱁􏱱$\\hat Y_i^B$ consisting of 􏰲the average of the individual predictions $\\hat Y_i$􏰹􏱁. You can think of the original noisy signal as consisting of 􏰲the bootstrpping prediction corrupted with additive noise: $Y_i^* = \\hat Y_i^B + \\epsilon_i$, with $\\epsilon_i \\sim \\mathcal(N)(0, n\\sigma_Y)$, where $\\sigma_Y$ is the standard deviation of the (unobservable) uncorrupted (predictable) time series $Y_i$ whic you estimated as $\\hat Y_i$. The bootstrap estimate itself can be thought of as the individual estimates $\\hat Y_i$ corrupted by the \"bootstrap noise\" coming from the dispersion in the individual bootstrap predictions: \n",
    "\n",
    "$$ \\hat Y_i^B = \\hat Y_i + \\epsilon_i^B $$\n",
    "\n",
    "with $\\epsilon_i^B \\sim \\mathcal{N} (0, \\sigma_B)$. Here $\\sigma_B$ is the standard deviation of the individual bootstrap predictions $\\hat Y_i$, which we can always write as: $\\sigma_B = n_B \\sigma_Y$, where usually $n_B \\ll n$. Note that, while $\\sigma_B$ is observable by taking the standard deviation of the individual model predictions, $\\sigma_Y$ is not directly observable. We write it in this way so that the unobservable $\\sigma_Y$􏰺􏰻 will drop out of our calculations, and we will shift the unobservable quantity to $n_B$􏰙􏱱. You now assume that the expected return of your trading strategy is proportional to the volatility of the bootstrapped prediction, $\\sigma_{Y^B}: r = \\lambda \\sigma_{Y^B}$, while the risk of the strategy is the volatility of the corrupted series 􏰹􏰲$Y_i^*$. Assuming that the bootstrap error 􏰼􏰲􏱱$\\epsilon_i^B$ is uncorrelated with the corrupted signal error $\\epsilon_i$ 􏰼, \n",
    "\n",
    "- (a) what is the Sharpe Ratio of your strategy in terms of $\\lambda,n,n_B$􏱜? \n",
    "- (b) By how much have your $R^2$􏱀􏰔 and SR increased by performing the bootstrap? (Compare your answer above to Problem 1 in HW#2 and Problem 1 in HW#3; you may exploit the fact that 􏰙􏱱$n_B \\ll n$ to simplify your answer.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "#### (a) Sharpe Ratio\n",
    "\n",
    "According to the description, the expected return of your trading strategy is proportional to the volatility of the bootstrapped prediction, \n",
    "\n",
    "$$r = \\lambda \\sigma_{Y^B} = \\lambda \\sqrt{\\mathrm{var}(\\hat Y_i + \\epsilon_i^B)} \n",
    "= \\lambda \\sigma_Y \\sqrt{1+n_B^2}$$\n",
    "\n",
    "assuming they are uncorrelated. On the other hand, the risk of the strategy is the volatility of the corrupted series\n",
    "\n",
    "$$ \\sqrt{\\mathrm{var}(Y_i^*)} = \\sqrt{\\mathrm{var}(\\hat Y_i^B + \\epsilon_i)} = \\sigma_Y \\sqrt{1+n_B^2+n^2}$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$ SR = \\frac{r}{\\sqrt{\\mathrm{var}(Y_i^*)}} = \\lambda \\sqrt{\\frac{1+n_B^2}{1+n_B^2+n^2}} \\cong \\lambda \\sqrt{\\frac{1+n_B^2}{1+n^2}}$$\n",
    "\n",
    "#### (b) Comparison\n",
    "\n",
    "The definition of $\\max R^2$ is\n",
    "\n",
    "$$\\max R^2 = 1 - \\frac{\\mathrm{var}(\\epsilon_i)}{\\mathrm{var}(Y_i^*)} \\cong \\frac{1+n_B^2}{1+n^2}$$\n",
    "\n",
    "which increased by $1+n_B^2$ compared to the result in HW2, and increased by $\\frac{1+n_B^2}{\\sqrt{1+n^2}}$ compared to the result in HW3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "\n",
    "Draw a diagram of a Recurrent FNN that would implement an ARMA(p,q) time-series prediction where p and q are the Autoregressive and Moving-Average orders, respectively, showing appropriate TDLs (if needed), and recursion (if needed). Explain all notation used, and describe how the network implements an ARMA(p,q) model. Why is Bootstrapping problematic when used with Time Series models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "We can use ANNs to approximate a time series by treating the time series as a single input pattern, but introducing Delays (D) via a Tapped Delay Line (TDL), and optionally by feeding the output back into the input layer via another TDL, as in Recurrent or Closed-Loop Networks:\n",
    "\n",
    "<img src='dynamic_ANN.png' width=400px>\n",
    "\n",
    "- The inputs can include the Target series itself $y_t$, 􏲴and/or the Output series 􏲴􏲹􏰓$\\hat y_t$.\n",
    "- This can approximate a general time series of the form:\n",
    "\n",
    "  $$ y_t = f(y_{t-1}, ..., y_{t-m}, \\hat y_{t-1}, ..., \\hat y_{t-n}) $$\n",
    "\n",
    "  where $f$ is the function to be approximated.\n",
    "  \n",
    "Regarding the graph notation, Inputs $p(t)$ are a (multi-dimensional) time series, inputs are presented sequentially but delayed $p$ time steps by the upper TDL. After that the inputs are going into the hidden layer $\\mathbf{LW^{1,1}}$, which generate the AR term\n",
    "\n",
    "$$ \\sum_{j=1}^p \\phi_j y_{t-j}$$\n",
    "\n",
    "On the other hand, the inputs also include the negetive output series 􏲴􏲹􏰓$-\\hat y_t$ itself, but lagged by $q$ time steps by the bottom TDL, after that the inputs are going into the hidden layer $\\mathbf{LW^{1,2}}$, which generate the MA term\n",
    "\n",
    "$$ - \\sum_{k=1}^q \\theta_k \\epsilon_{t-k} $$\n",
    "\n",
    "To implement ARMR(p,q), simply let the activation functions $f^1$ and $f^2$ be linear, particularly the Hidden-Layer.\n",
    "\n",
    "Bootstrapping is problematic when used with time series models because sampling does not consider the ordering of the time series."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
